---
title: "Bayesian Statistics"
subtitle: "A Conceptual and Practical Introduction"
author: "Fabian Dablander"
runtime: shiny
bibliography: bibliography.bib
output:
  ioslides_presentation:
    css: styles.css
    smaller: yes
    transition: faster
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, dev.args = list(bg = 'transparent'), fig.align = 'center')
```

\usepackages{csquotes}
\newcommand{\param}{\mathbf{\theta}}
\newcommand{\dat}{\textbf{y}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

## Motivation
- How old is statistics?
- Who invented the *p* value?
- What is Bayes' theorem?
- add a picture of stats tests (rethinking)
- add a picture of wansink
- add some stuff about the post-factual world
- statistics and drawing sound inferences from data is important as ever!
- create an exercise sheet


## Motivation
![](images/statistical-tests.png)

## Learning outcome
- **Oblique**
    - Get a feeling for *statistical modeling*
    - Get a feeling for statistics as a (young) discipline
- **Concrete**
    - Be able to state the differences between classical and Bayesian statistics
    - Be able to apply simple Bayesian models to your data
- **Wish for**
    - Become hungry for statistical developments
    - Read the linked resources at https://github.com/fdabl/Intro-Stats to dive deeper


## Outline
- **1a.** History of statistics
    - Relation to science
    - Controversies and enigmatic key players
    
<span style = "color:white"></span>

- **1b.** Introduction to probability
    - Why most published research is false
    - Frequentist versus Bayesian interpretation

<span style = "color:white"></span>
    
- **2.** Statistical models
    - Bread and butter
    - (Re)introduction *p*-values, confidence intervals, statistical power
    

## Outline
- **3.** Introduction to Bayesian statistics
    - Parameter estimation, model comparison, model prediction
    
<span style = "color:white"></span>

- **4a.** Some applications
    - Simulator sickness and age
    - Harry Potter and Personality
    - Markov chain Monte Carlo with People
    
<span style = "color:white"></span>

- **4b.** Outlook
    - Where is Bayes going?
    - Where is JASP going?
    - Where are *you* going?

# History of statistics
## Overview

# Introduction to Probability

## Probability theory
- Is based on set theory
- $\mathcal{S} = \{0, 1, 2, 3\}$ denotes the *sample space*
- $\mathcal{A} = \{0, 1\}$ and $\mathcal{B} = \{1, 2\}$ denote events; $\mathcal{A}, \mathcal{B} \subset \mathcal{S}$

$$
\begin{align*}
\mathcal{A^c} &= \{2, 3\} \\
\mathcal{B^c} &= \{0, 3\} \\
\mathcal{A} \cap \mathcal{B} &= \{1\} \\
\mathcal{A} \cup \mathcal{B} &= \{0, 1, 2\}
\end{align*}
$$

## Probability theory
- Naive view assumes equally likely outcomes, resulting in 

$$
P(A) = \frac{|A|}{|S|}
$$

- where $P(.)$ is a function that takes a set and returns a real number on the interval [0, 1]
- In order to be a valid probability function, the function must satisfy certain constraints


## Probability theory
- Kolmogorov's axioms (1933)

$$
\begin{align*}
P(\mathcal{S}) = 1, \,  P(\emptyset) = 0 \\
P(\bigcup^{\infty}_{i = 1}\mathcal{A}_i) = \sum^{\infty}_{i=1}\mathcal{A}_i
\end{align*}
$$
<center>
<img src="images/venn-diagram.png" </img>
</center>

## Probability theory
- Some neat properties result

$$
\begin{split}
P(\mathcal{A}^c) &= 1 - P(\mathcal{A}^c) \\
P(\mathcal{A \cup B}) &= P(\mathcal{A}) + P(\mathcal{B}) - P(\mathcal{A \cap B}) \\
P(\mathcal{A}) &\leq P(\mathcal{B}) \, \, \, \, \, \, \, \text{  if   } \mathcal{A} \subset \mathcal{B}
\end{split}
$$

## How likely is it that today is somebody's birthday?
- The sample space is $\mathcal{S} = \{1, 2, \ldots, 365\}$
- There are $k$ people in the room; therefore

$$
P(\text{somebody's birthday}) = k \cdot \frac{1}{365}
$$

- How likely is it that at least two people share birthdays?

## Birthday Problem
- Use $P(A^c) = 1 - P(A)$ to simplify the problem

$$
\begin{split}
P(\text{two or more people share birthdays}) &= 1 - P(\text{nobody shares birthdays}) \\[1ex]
                                             &= 1 - \frac{365 \cdot (365 - 1) \cdot \ldots \cdot (365 - k + 1)}{365^k}
\end{split}
$$

```{r, echo = FALSE, fig.width = 6, fig.height = 4}
library('ggplot2')
theme_set(papaja::theme_apa())

k <- seq(0, 50)
f <- function(k) 1 - ((choose(365, k) * factorial(k)) / 365^k)
d <- data.frame(k = k, prob = f(k))

ggplot(d, aes(x = k, y = prob)) +
  geom_line() +
  ylab('Probability') +
  geom_hline(yintercept = .5, linetype = 'dashed')
```


## Why most published research is false

+--------------------------+---------------+------------------+
|                          |$\mathcal{H}_0$|  $\mathcal{H}_1$ |
+--------------------------+---------------+------------------+
| reject $\mathcal{H}_0$   | $\alpha$      |    $1 - \beta$   |
+--------------------------+---------------+------------------+
| keep $\mathcal{H}_0$     | 1 - $\alpha$  |      $\beta$     |
+--------------------------+---------------+------------------+

## Why most published research is false
- What is the probability that my hypothesis is true, given that I have observed $p < \alpha$?

$$
P(H_1|p < \alpha) = \frac{P(p < \alpha|H_1)P(H_1)}{P(p < \alpha|H_1)P(H_1) + P(p < \alpha|H_0)P(H_0)}
$$


## Why most published research is false {.flexbox .vcenter .emphasize}
[Demo](http://shinyapps.org/apps/PPV/)

## Probability theory
- It is a branch of mathematics based on set theory, and as such uncontroversial
- However, there are different interpretations of probability
    - *Frequentist view:* Probability is long-running frequency
    - *Bayesian view:* Probability as degree of belief


# Statistical models
## Statistical models
- Say I have a set of $n = 100$ observations $d = \{x_1, x_2, x_3, \ldots, x_n\}$
- You ask me about the data; what can I tell you?
```{r, echo = FALSE}
set.seed(1774)
x <- rnorm(100, 100, 10)
```

```{r}
x
```

## Statistical models
- Introduce a statistical model to reduce complexity
- A statistical model describes the relationship of one or more **latent parameters** to the observations

$$
f(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \big (-\frac{1}{2\sigma} (x - \mu)^2 \big)
$$

```{r, echo = FALSE}
mu <- mean(x)
sigma <- sd(x)

ggplot(data.frame(x = x), aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 20, fill = 'grey46', col = 'black') +
  stat_function(fun = function(y) dnorm(y, mean = mu, sd = sigma), col = 'firebrick', size = 1.5)
```

## Statistical models
- I used a normal distribution for the data
- Now I can describe the data with just two numbers --- the mean $\mu$ and the standard deviation $\sigma$

```{r}
c(mean(x), sd(x))
```

- But there are other possible statistical models I could use!


## Statistical models
- Statistical models need not be appropriate; in fact, they can be horribly misspecified!

$$
f(x; a, b) = \frac{1}{b - a}
$$
```{r, echo = FALSE}
ggplot(data.frame(x = x), aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 20, fill = 'grey46', col = 'black') +
  stat_function(fun = function(y) dunif(y, min = 50, max = 150), col = 'firebrick', size = 1.5)
```

## Statistical models: Notation
- Statistical models can be specified as **directed acyclic graphs**

<span style = "color: white"></span>

<center>
<img src="images/uniform-normal-models.png" />
</center>


## Bread and butter
- A slice of bread with butter falls on the floor
- It can either land with butter up (**0**) or butter down (**1**)

<span style = "color: white"></span>

- Questions
    - <span style = "color:red">What is the propensity of the bread to fall on the floor with the butter side down?</span>
    - <span style = "color:red">How do I describe the data-generating process? Any suggestions?</span>


## Bread and butter
- First, note that the outcome is binary. The random variable describing the outcome is $X = \{0, 1\}$
- A good, tried suggestion would be the following statistical model

$$
\begin{align*}
f(x; \theta) &= \theta^x (1 - \theta)^{1 - x} \\[2ex]
             &= \begin{cases} \theta & \text{if } x = 1 \\ 1 - \theta & \text{otherwise} \end{cases}
\end{align*}
$$
- where $\theta$ is the latent parameter whose value influences the outcome


## Statistical models: Bread and butter
- How do I describe $n$ events?
- I make a <span style = "color:red">very contentious assumption</span>: **independent and identically distributed samples**
- Assume I have $n = 20$ data points $d = \{x_1, x_2, x_3, \ldots, x_n\}$

## Statistical models: Bread and butter

$$
\begin{align*}
f(x; \theta) &\stackrel{\text{i.i.d.}}{=} f(x_1; \theta) \cdot f(x_2; \theta) \cdot \ldots \cdot f(x_n; \theta) \\
             &= \prod^n_{i=1} f(x_i; \theta) \\
             &= \prod^n_{i=1} \theta^{x_i} (1 - \theta)^{1 - x_i} \\
             &= \theta^{\sum^n_{i = 1} x_i} (1 - \theta)^{\sum^n_{i = 1} 1 - x_i}
\end{align*}
$$

- We don't care about the outcome order; e.g., whether $x = \{1, 0 \}$ or $x = \{0, 1\}$


## Binomial likelihood
- Neglecting the order, we only model the number of falls with butter on the floor
- Introducing a new random variable $Y$

$$
Y = \sum_{i=1}^n x_i
$$

- and noting that there are

$$
n\cdot(n - 1)\cdot \ldots \cdot (n - y + 1) = \frac{n!}{y!(n - y)!} = {n \choose y}
$$

- possible ways of obtaining $Y = y$ successes yields the **Binomial likelihood**


## Binomial likelihood

$$
\begin{align*}
f(x; \theta) &\stackrel{\text{i.i.d.}}{=} f(x_1; \theta) \cdot f(x_2; \theta) \cdot \ldots \cdot f(x_n; \theta) \\
             &= \prod^n_{i=1} f(x_i; \theta) \\
             &= \prod^n_{i=1} \theta^{x_i} (1 - \theta)^{1 - x_i} \\
             &= \theta^{\sum^n_{i = 1} x_i} (1 - \theta)^{\sum^n_{i = 1} 1 - x_i} \\[4ex]
f(y; \theta, n) &= {n \choose y} \theta^{\sum^n_{i = 1} x_i} (1 - \theta)^{\sum^n_{i = 1} 1 - x_i} \hspace{3em} \text{Neglecting order} \\[1ex]
                &= {n \choose y} \theta^{y} (1 - \theta)^{n - y} \hspace{7em} \text{Introducing  } Y = \sum^n_{i=1} x_i \\
\end{align*}
$$

## Binomial likelihood
![](images/binomial-model.png)


## Binomial likelihood
- $\theta$ describes how the data will turn out
- In other words: the likelihood is a distribution for the data

```{r, echo = FALSE}
n <- 20
xx <- seq(n)
d <- data.frame(
  x = rep(xx, 3),
  theta = factor(rep(c(.2, .5, .8), each = n)),
  y = c(dbinom(xx, n, prob = .2), dbinom(xx, n, prob = .5), dbinom(xx, n, prob = .8))
)

ggplot(d, aes(x = x, y = y, group = theta)) +
  geom_bar(stat = 'identity', aes(fill = theta)) +
  facet_wrap(~ theta) +
  xlab('y') +
  ylab('Density') +
  scale_fill_discrete(name = expression(theta))
```

## Binomial likelihood
```{r, echo = FALSE, height = 3, width = 3}
library('shiny')
library('latex2exp')

mathjax_URL <- 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full'

shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        tags$head(
          tags$script(src = mathjax_URL, type = 'text/javascript'),
          tags$script("MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});", type = 'text/x-mathjax-config')
    ),
        sliderInput("p", label = HTML('$$\\theta$$'),
                    min = 0, max = 1, value = 0.5, step = 0.01),
        sliderInput("n", label = "n",
                    min = 1, max = 100, value = 20, step = 1)
        ),
      mainPanel(
        plotOutput("LRplot", height="400px")
        )
      )
   )),
   server = function(input, output) {
      plot_dat <- function(n, p) {
        x <- seq(n)
        y <- dbinom(x, n, p)
        d <- data.frame(
          x = x,
          theta = p,
          y = y / max(y)
        )
        
        ggplot(d, aes(x = x, y = y)) +
          geom_bar(stat = 'identity') +
          xlab('y') +
          ylab('Density') +
          ggtitle(TeX(paste0('Data distribution for ', '$\\theta$ = ', round(p, 2)))) +
          theme(plot.title = element_text(hjust = .5))
      }
      
      output$LRplot <- renderPlot({
        plot_dat(input$n, input$p)
      })
    }
)
```

    
## Bread and butter
- I want to learn $\theta$ --- what are good values for it, given the data below?

```{r, echo = FALSE, fig.width = 4, figh.height = 2}
set.seed(1774)

x <- rbinom(20, 1, prob = .7)
y <- sum(x)
n <- length(x)

ggplot(data.frame(x = x), aes(x = x)) +
  geom_bar()
```


## Bread and butter
- **How likely is the data given certain parameter values?**
- Let's compare $\theta = .5$ and $\theta = .75$ on our flipping data ($y = 15, n = 20$)

$$
\begin{align*}
f(Y = 15|n = 20, \theta = .5)  &= {20 \choose 15} .50^{15} (1 - .50)^{20 - 15} \approx 0.0147 \\[1ex]
f(Y = 15|n = 20, \theta = .75) &= {20 \choose 15} .75^{15} (1 - .75)^{20 - 15} \approx 0.2033
\end{align*}
$$

## Bread and butter
- Let's plot the likelihood of the data as a function of $\theta$

```{r, echo = FALSE}
ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(fun = function(theta) dbinom(sum(x), length(x), prob = theta)) +
  ylab('Likelihood') +
  xlab(expression(theta)) +
  annotate('text', x = .15, y = .18, label = 'y = 15\nn = 20', size = 6)
```


## Bread and butter
```{r, echo = FALSE}
p1 <- .75
p2 <- .5
llh.p1 <- dbinom(y, n, p1)
llh.p2 <- dbinom(y, n, p2)

ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(fun = function(theta) dbinom(y, n, prob = theta)) +
  geom_segment(x = p1, y = -1, xend = p1, yend = llh.p1, aes(col = 'blue'), linetype = 'dashed') +
  geom_segment(x = p2, y = -1, xend = p2, yend = llh.p2, aes(col = 'red'), linetype = 'dashed') +
  geom_point(x = p1, y = llh.p1, size = 2) +
  geom_point(x = p2, y = llh.p2, size = 2) +
  scale_colour_manual(name = '', values = c('red', 'blue'),
                      labels = c(expression(hat(theta)[1]), expression(hat(theta)[2]))) +
  ylab('Likelihood') +
  xlab(expression(theta))
```

## Likelihood ratios
```{r, echo = FALSE, height = 3, width = 3}
shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        tags$head(
          tags$script(src = mathjax_URL, type = 'text/javascript'),
          tags$script("MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});", type = 'text/x-mathjax-config')
    ),
        sliderInput("p1", label = HTML('$$\\theta_1$$'),
                    min = 0, max = 1, value = 0.75, step = 0.01),
        sliderInput("p2", label = HTML('$$\\theta_2$$'),
                    min = 0, max = 1, value = 0.5, step = 0.01),
        sliderInput("y", label = "y",
                    min = 1, max = 50, value = 15, step = 1),
        sliderInput("n", label = "n",
                    min = 1, max = 50, value = 20, step = 1)
        ),
      mainPanel(
        plotOutput("LRplot", height="400px")
        )
      )
   )),
   server = function(input, output) {
      plot.LR <- function(y, n, p1, p2) {
        # # adapted from blog mentioned above
        # MLE <- dbinom(k, N, k/N)
        # L1 <- dbinom(k, N, prob = p1) / MLE
        # L2 <- dbinom(k, N, prob = p2) / MLE
        # 
        # curve((dbinom(k, N, x) / max(dbinom(k, N, x))), xlim = c(0, 1),
        #       ylab = "Likelihood", xlab = "Probability of correct answer",
        #       las = 1, main = "Likelihood function for binomials", lwd = 3,
        #       cex.axis = 1.5, cex.lab = 1.5, cex.main = 1.5)
        # 
        # points(p1, L1, cex = 2, pch = 21, bg = "cyan")
        # points(p2, L2, cex = 2, pch = 21, bg = "cyan")
        # lines(c(p1, p2), c(L1, L1), lwd = 3, lty = 2, col = "cyan")
        # lines(c(p2, p2), c(L1, L2), lwd = 3, lty = 2, col = "cyan")
        # abline(v = k/N, lty = 5, lwd = 1, col = "grey73")
        llh.p1 <- dbinom(y, n, p1)
        llh.p2 <- dbinom(y, n, p2)
        
        ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
          stat_function(fun = function(theta) dbinom(y, n, prob = theta)) +
          geom_segment(x = p1, y = -1, xend = p1, yend = llh.p1, aes(col = 'blue'), linetype = 'dashed') +
          geom_segment(x = p2, y = -1, xend = p2, yend = llh.p2, aes(col = 'red'), linetype = 'dashed') +
          geom_point(x = p1, y = llh.p1, size = 2) +
          geom_point(x = p2, y = llh.p2, size = 2) +
          scale_colour_manual(name = '', values = c('red', 'blue'),
                              labels = c(expression(hat(theta)[1]), expression(hat(theta)[2]))) +
          ylab('Likelihood') +
          xlab(expression(theta)) +
          ggtitle(TeX(paste0('Likelihood ratio of ',
                             '$\\frac{f(y; n, \\theta_1)}{f(y; n, \\theta_2)}$ = ',
                             get.LR(y, n, p1, p2)))) +
          theme(plot.title = element_text(hjust = .5))
      }
      
      get.LR <- function(y, n, p1, p2) {
        MLE <- dbinom(y, n, k/N)
        L1 <- dbinom(y, n, prob = p1) / MLE
        L2 <- dbinom(y, n, prob = p2) / MLE
        round(L1 / L2, 2)
      }
      
      output$LRplot <- renderPlot({
        plot.LR(input$y, input$n, input$p1, input$p2)
      })
    }
)
```


# Classical and Bayesian statistics
## A neutral Bird's eye view
  Ronald Fisher                     Neyman-Pearson                       Bayesian
----------------------              ---------------------------          ---------------------------
Probability as long-run frequency   Probability as long-run frequency    Probability as degree of belief
*p*-value                           *p*-value                            Bayes factors
$\alpha$                            $\alpha, \beta$                      -
Parameters are fixed                Parameters are fixed                 Parameters are random
Pre-data                            Pre-data                             Post-data
Epistemic approach                  Behavioural approach                 Epistemic approach
Various estimators                  Various estimators                   Bayes' rule
-                                   Confidence Intervals                 Credible intervals
Point estimates                     Point estimates                      Posterior distributions


## A polemic Bird's eye view
 Classical Statistics |     Bayesian Statistics
---------------------:|:---------------------------
Ad-hoc                |    Axiomatic
Incoherent            |    Coherent
Paradoxical           |    Intuitive
Irrational            |    Rational
Ugly                  |    Pretty
Irrelevant            |    Relevant
what's taught         |    what's not taught

<div id="contrast">borrowed from [EJ Wagenmakers](https://docs.google.com/file/d/0B-Ww24m3ZkEyMEpudlVsX3pRVzA/edit)</div>

## Practical example
- **Data example**
    - We observe $n = 20$ pieces of bread with butter falling down
    - $y = 15$ land on the floor with the butter down
    - Questions
        - <span style = 'color: red'>Estimation</span>: What are likely values for $\theta$?
        - <span style = 'color: red'>Testing</span>: Is bread with butter more likely to fall butter down?
        - <span style = 'color: red'>Prediction</span>: How likely is the next piece to fall butter down?
        
Problem           Classical statistics     Bayesian statistics
-----------       --------------------     --------------------
Estimation        Maximum Likelihood, CIs  Posterior distribution (**Bayes' rule**)
Testing           *p*-value                Bayes factor (**Bayes' rule**)
Prediction        Plug-in Estimate         Account for uncertainty using **Bayes' rule**


# Classical statistics

## Bread and Butter: Estimation
- Many different estimators, but most popluar is maximum likelihood estimate; $\hat{\theta} = \frac{y}{n}$

```{r, echo = FALSE}
y <- 15
n <- 20

ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(fun = function(theta) dbinom(y, n, prob = theta)) +
  geom_segment(x = y/n, xend = y/n, y = -1, yend = dbinom(y, n, prob = y/n), color = 'red', linetype = 'dashed') +
  geom_point(x = y/n, y = dbinom(y, n, prob = y/n), size = 2) +
  ylab('Likelihood') +
  xlab(expression(theta))
```


## Bread and Butter: Estimation
- The main assumption of classical statistics in two sentences

> We assume that we sample repeatedly from a population; each time we compute an estimate based on the sample. The distribution of these estimates --- the sampling distribution --- gives the precision of the specific estimate.

- In other words

> We conduct an experiment. We then assume that we conduct exactly the same experiment over and over again. For
> each such experiment, we compute a statistic (say the mean). Plotting these statistics yields the sampling distribution
> (of the mean).

## Bread and Butter: Estimation
```{r}
conduct_experiment <- function(y, n) rbinom(n, 1, prob = y / n)
replicate(10, conduct_experiment(y, n))
```

## Bread and Butter: Estimation
```{r}
sampling_distribution <- apply(replicate(10000, conduct_experiment(y, n)), 2, mean)
```

```{r, echo = FALSE, fig.width = 7, fig.height = 5}
title <- TeX(paste0('Sampling distribution of ', '$\\theta$'))
hist(sampling_distribution, col = 'grey70', xlab = expression(theta), main = title, xlim = c(0, 1))
```


## Confidence intervals
```{r, echo = FALSE}
simulate_cis <- function(x, n, times = 100, n_boot = 1000) {
    p <- sum(x) / n
    cis <- matrix(NA, nrow = times, ncol = 2)
    
    for (i in seq(times)) {
      dat <- sample(c(0, 1), size = n, prob = c(1-p, p), replace = TRUE)
      
      p.hat <- mean(dat)
      se <- sd(dat) / sqrt(n)
      
      cis[i, 1] <- p.hat - 1.96 * se
      cis[i, 2] <- p.hat + 1.96 * se
    }
    
    within <- apply(cis, 1, function(row) row[1] < p && row[2] > p)
    cbind(cis, within, p)
}

plot_cis <- function(cis, times) {
  p <- cis[1, 4]
  x <- seq(0, times)
  z <- seq(0, 1, 1/times)
  
  plot(x, z, type = 'n', xlab = 'i-th Experiment',
       ylab = TeX('$\\theta$'), main = TeX(paste0('95% CIs for ', '$\\theta$')))
  
  abline(p, 0, lwd = 3)
  
  arrows(x0 = x, y0 = cis[, 1], x1 = x, length = 0, lwd = 1.2,
         y1 = cis[, 2], col = ifelse(cis[, 3], 'black', 'red'))
}

times <- 100
cis <- simulate_cis(y, n, times = times)
```

```{r, echo = FALSE}
plot_cis(cis, times)
```

## Confidence intervals {.flexbox .vcenter .emphasize}
[Demo](http://rpsychologist.com/d3/CI/)

# Bayesian statistics

## Bayesian estimation
- Use Bayes' rule
$$
\underbrace{p(\theta|y)}_{\text{Posterior}} = \frac{\overbrace{f(y; \theta)}^{\text{Likelihood}}\overbrace{p(\theta)}^{\text{Prior}}}{\underbrace{p(y)}_{\text{Marginal Likelihood}}}
$$

- where, applying the law of total probability

$$
p(y) = \int_{\theta} f(y; \theta)p(\theta) \mathrm{d}\theta
$$

- Use a Beta distribution as our prior

$$
p(\theta|\alpha, \beta) = \frac{1}{\text{Beta}(\alpha, \beta)} \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}
$$

## Beta distributions and betting
```{r, echo = FALSE, height = 3, width = 3}
shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        tags$head(
          tags$script(src = mathjax_URL, type = 'text/javascript'),
          tags$script("MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});", type = 'text/x-mathjax-config')
    ),
        sliderInput("a", label = 'a',
                    min = .5, max = 20, value = 1, step = .5),
        sliderInput("b", label = "b",
                    min = .5, max = 20, value = 1, step = .5)
        ),
      mainPanel(
        plotOutput("LRplot", height="400px")
        )
      )
   )),
   server = function(input, output) {
      plot_dat <- function(a, b) {
        ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
          stat_function(fun = function(x) dbeta(x, a, b)) +
          xlab(expression(theta)) +
          ylab('Density') +
          ggtitle(paste0('Beta(', a, ', ', b, ')', ' with mean ', round(a / (a + b), 2))) +
          theme(plot.title = element_text(hjust = .5))
      }
      
      output$LRplot <- renderPlot({
        plot_dat(input$a, input$b)
      })
    }
)
```


## Bayesian graphical model
![](images/binomial-model-bayes.png)


## Bayesian estimation
$$
\begin{split}
p(\theta|y) &= \frac{f(y; \theta)p(\theta)}{\int_{\theta}f(y; \theta)p(\theta)\mathrm{d}\theta} \\[1ex]
p(\theta|y) &= \frac{{n \choose y} \theta^y (1 - \theta)^{n - y} \frac{1}{\mathcal{B}(a, b)} \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}}{\int_{\theta}{n \choose y} \theta^y (1 - \theta)^{n - y} \frac{1}{\mathcal{B}(a, b)} \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}\mathrm{d}\theta}
\end{split}
$$
- with some rearranging, we find that

$$
p(\theta|y) = \frac{1}{\text{Beta}(y + \alpha, n - y + \beta)} \theta^{y + \alpha - 1}(1 - \theta)^{n - y + \beta - 1}
$$

## Bayesian updating
```{r, echo = FALSE, height = 3, width = 3}
library('shiny')
# the code (with a little cleaning up) for the visualisations is from
# http://alexanderetz.com/2015/07/25/understanding-bayes-updating-priors-via-the-likelihood/
# Alex Etz runs a really nice blog -- go check it out!

shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        sliderInput("a", label = "a", min = 1, max = 50, value = 1, step = 1),
        sliderInput("b", label = "b", min = 1, max = 50, value = 1, step = 1),
        sliderInput("k", label = "y", min = 1, max = 50, value = 15, step = 1),
        sliderInput("N", label = "n", min = 1, max = 50, value = 20, step = 1),
        htmlOutput("BF_10")
        ),
      mainPanel(
        plotOutput('update_plot', height='400px')
        )
      )
    )
  ),
  server = function(input, output) {
        update_plot <- function(a = 1, b = 1, k = 0, N = 0, null = NULL, CI = NULL, ymax = 'auto') {
          x <- seq(.001, .999, .001) ## set up for creating the distributions
          y1 <- dbeta(x, a, b) # data for prior curve
          y3 <- dbeta(x, a + k, b + N - k) # data for posterior curve
          y2 <- dbeta(x, 1 + k, 1 + N - k) # data for likelihood curve, plotted as the posterior from a beta(1,1)
          y.max <- ifelse(is.numeric(ymax), ymax, 1.25 * max(y1, y2, y3, 1.6))
          title <- paste0('Beta(', a, ', ', b, ')', ' to Beta(', a + k, ', ', b + N - k, ')')
          
          plot(x, y1, xlim = c(0, 1), ylim = c(0, y.max), type = 'l', ylab = 'Density', lty = 2,
               xlab = TeX('$\\theta$'), las = 1, main = title, lwd=3,
               cex.lab = 1.5, cex.main = 1.5, col = 'skyblue', axes = FALSE)
          
          axis(1, at = seq(0, 1, .2)) #adds custom x axis
          axis(2, las = 1) # custom y axis
          
          if (N != 0) {
              # if there is new data, plot likelihood and posterior
              lines(x, y2, type = 'l', col = 'darkorange', lwd = 2, lty = 3)
              lines(x, y3, type = 'l', col = 'darkorchid1', lwd = 5)
              legend('topleft', c('Prior', 'Posterior', 'Likelihood'),
                     col = c('skyblue', 'darkorchid1', 'darkorange'), 
                     lty = c(2, 1, 3), lwd = c(3, 5, 2), bty = 'n',
                     y.intersp = 1, x.intersp = .4, seg.len =.7)
                  
              ## adds null points on prior and posterior curve if null is specified and there is new data
              if (is.numeric(null)) {
                      ## Adds points on the distributions at the null value if there is one and if there is new data
                      points(null, dbeta(null, a, b), pch = 21, bg = 'blue', cex = 1.5)
                      points(null, dbeta(null, a + k, b + N - k), pch = 21, bg = 'darkorchid', cex = 1.5)
                      abline(v=null, lty = 5, lwd = 1, col = 'grey73')
                      ##lines(c(null,null),c(0,1.11*max(y1,y3,1.6))) other option for null line
                }
          }
          
          ## Specified CI% but no null? Calc and report only CI
          if (is.numeric(CI) && !is.numeric(null)) {
                CI.low <- qbeta((1 - CI)/2, a + k, b + N - k)
                CI.high <- qbeta(1 - (1 - CI)/2, a + k, b + N - k)
                
                SEQlow <- seq(0, CI.low, .001)
                SEQhigh <- seq(CI.high, 1, .001)
                
                ## Adds shaded area for x% Posterior CIs
                cord.x <- c(0, SEQlow, CI.low) ## set up for shading
                cord.y <- c(0, dbeta(SEQlow, a + k, b + N - k), 0) ## set up for shading
                polygon(cord.x, cord.y, col='orchid', lty= 3) ## shade left tail
                cord.xx <- c(CI.high, SEQhigh, 1) 
                cord.yy <- c(0, dbeta(SEQhigh, a + k, b + N - k), 0)
                polygon(cord.xx, cord.yy, col='orchid', lty=3) ## shade right tail
                return(list('Posterior CI lower' = round(CI.low, 3),
                            'Posterior CI upper' = round(CI.high, 3)))
          }
          
          ## Specified null but not CI%? Calculate and report BF only 
          if (is.numeric(null) && !is.numeric(CI)){
              null.H0 <- dbeta(null, a, b)
              null.H1 <- dbeta(null, a + k, b + N - k)
              CI.low <- qbeta((1 - CI)/2, a + k, b + N - k)
              CI.high <- qbeta(1 - (1 - CI)/2, a + k, b + N - k)
              return(list('BF01 (in favor of H0)' = round(null.H1/null.H0, 3),
                          'BF10 (in favor of H1)' = round(null.H0/null.H1, 3)))
          }
          
          ## Specified both null and CI%? Calculate and report both
          if (is.numeric(null) && is.numeric(CI)){
                  null.H0 <- dbeta(null, a, b)
                  null.H1 <- dbeta(null, a + k, b + N - k)
                  CI.low <- qbeta((1 - CI)/2, a + k, b + N - k)
                  CI.high <- qbeta(1 - (1 - CI)/2, a + k, b + N - k)
                  
                  SEQlow <- seq(0, CI.low, .001)
                  SEQhigh <- seq(CI.high, 1, .001)
                  
                  ## Adds shaded area for x% Posterior CIs
                  cord.x <- c(0, SEQlow, CI.low) ## set up for shading
                  cord.y <- c(0, dbeta(SEQlow, a + k, b + N - k), 0) ## set up for shading
                  polygon(cord.x, cord.y, col = 'orchid', lty = 3) ## shade left tail
                  cord.xx <- c(CI.high, SEQhigh, 1) 
                  cord.yy <- c(0, dbeta(SEQhigh, a + k, b + N - k), 0)
                  polygon(cord.xx, cord.yy, col = 'orchid', lty = 3) ## shade right tail
                  return(list('BF01 (in favor of H0)' = round(null.H1/null.H0, 3),
                              'BF10 (in favor of H1)' = round(null.H0/null.H1, 3),
                              'Posterior CI lower' = round(CI.low, 4),
                              'Posterior CI upper' = round(CI.high, 3)))
          }
        }
        output$update_plot <- renderPlot({
          null <- .5
          update_plot(input$a, input$b, input$k, input$N, null = null)
        })
        
        output$BF_10 <- renderText({
          a <- input$a
          b <- input$b
          k <- input$k
          N <- input$N
          label <- 'BF<span style="font-size: .6em;">10</span>: '
          BF10 <- 1 / (dbeta(.5, a + k, b + N - k) / dbeta(.5, a, b))
          paste(label, round(BF10, 3))
        })
  }
)
```

## Bayesian model comparison
- In classical statistics, we test

$$
H_0: \theta = .5 \\
H_1: \theta \neq .5 \\
$$

- but the model which instantiates $H_1$ makes no predictions whatsoever!
- We could use likelihoods to compare the hypotheses, such as

$$
\begin{split}
H_0&: \theta = .5 \\
H_1&: \theta = .75
\end{split}
$$

- but this is too demanding; why .75? Why not .80? Or .30?


## Bayesian model comparison
- We are uncertain about alternative values for $\theta$
- Bayes solution: quantify our uncertainty with a prior distribution

$$
\begin{split}
H_0&: \theta = .5 \\
H_1&: \theta \sim \mathcal{Beta}(\alpha, \beta)
\end{split}
$$

- Now both models make predictions!
- Bayesian model comparison pits the predictive accuracy of two models against each other

- For $M_0$, this results in
$$
\begin{split}
p(y|M_0) &= \int_{\theta} p(y, \theta|M_0)\mathrm{d}\theta \\[1ex]
         &= \int_{\theta} p(y|M_0, \theta)p(\theta|M_0)\mathrm{d}\theta \\[1ex]
         &= f(y|M_0, \theta = .5) \\[2ex]
         &= {20 \choose 15} .50^{15} (1 - .50)^{20 - 15} \approx 0.0147
\end{split}
$$

## Bayesian model comparison
- For $M_1$, it's a bit more complicated

$$
\begin{split}
p(y|M_1) &= \int_{\theta} p(y, \theta|M_1)\mathrm{d}\theta \\[1ex]
         &= \int_{\theta} p(y|M_1, \theta)p(\theta|M_1)\mathrm{d}\theta \\[1ex]
         &= \int_{\theta} {20 \choose 15} \theta^{15} (1 - \theta)^{20 - 15} \frac{1}{\text{Beta}(a, b)}\theta^{\alpha - 1}(1 - \theta)^{\beta - 1}\mathrm{d}\theta \\[1ex]
         &\approx \frac{1}{N} \sum_{\theta \sim \mathcal{Beta}(\alpha, \beta)}^N {20 \choose 15} \theta^{15} (1 - \theta)^{20 - 15} \hspace{3em} \text{Monte Carlo integration} \\[1ex]
         &\approx 0.047
\end{split}
$$

## Bayesian model comparison
- How do we update our beliefs across models?
$$
\begin{split}
\underbrace{\frac{p(M_0 \mid y)}{p(M_1 \mid y)}}_{\text{posterior odds}} &=
\underbrace{\frac{p(y \mid M_0)}{p(y \mid M_1)}}_{\text{Bayes factor}} \, \cdot \underbrace{\frac{p(M_0)}{p(M_1)}}_{\text{prior odds}} \\[1ex]
\frac{p(M_0|y)}{p(M_1|y)} &= \frac{.0147}{.0477} \cdot \frac{1/2}{1/2}
\approx .31
\end{split}
$$

```{r, echo = FALSE, fig.width = 5, fig.height = 3}
post_m0 <- 1 / (1 + .31)
post_m1 <- 1 - post_m0

d <- data.frame(p = c(1/2, post_m0, 1/2, post_m1),
                m = factor(rep(c('M_0', 'M_1'), each = 2)),
                distribution = relevel(factor(rep(c('prior', 'posterior'), 2)), ref = 'prior'))

ggplot(d, aes(x = m, y = p, fill = distribution)) +
  geom_bar(stat = 'identity') +
  xlab('Model') +
  ylab('Posterior probability') +
  facet_wrap(~ distribution) +
  theme(legend.position = 'none')
```


## Bayes factor
- is the non-plus-ultra of model comparison
    - complex models can predict many data patterns
    - thus spread out their prior probability
    - this gets factored into the marginal likelihood and decreases it
    - instantiates an **automatic Ockham's razor**
\
\
- looks at the functional form of the model, compared to
    - $AIC = -2 \log p(\textbf{y}|\hat \theta) + 2\cdot k$
    - $BIC = -2 \log p(\textbf{y}|\hat \theta) + \log n \cdot k$

## Thermometer of evidence
<center>
<img src="images/bayes-factor-labels.png" />
</center>

## Lindley's Paradox
```{r}
k <- 49581
N <- 98451
```

<div style = "float:left; width:45%;">
**$p$-value**

```{r, echo = FALSE}
binom.test(k, N)$p.value
```
</div>
<div style = "float:right; width:45%;">

**$BF_{01}$**

```{r, echo = FALSE}
dbeta(0.5, k + 1, N - k + 1)
```  
</div>


## Bayesian prediction
- What is the probability that the next slice of bread will fall butter down?

$$
\begin{split}
p(y' = 1|d) &= \int_{\theta} p(y' = 1, \theta|d) \mathrm{d}\theta \\
            &= \int_{\theta} p(y' = 1|d, \theta) p(\theta|d) \mathrm{d}\theta \\[1ex]
            &= \int_{\theta} \theta^{y'} (1 - \theta)^{1 - y'} \frac{1}{\text{Beta}(y + \alpha, n - y + \beta)} \theta^{\alpha + y - 1}(1 - \theta)^{n - y + \beta - 1}\mathrm{d}\theta
\end{split}
$$

# Some applications

## Simulator sickness and age
```{r, echo = FALSE}
ss_dat <- read.csv('../data/ss_dat.csv')

ggplot(ss_dat, aes(x = age, y = diff_ssq)) +
  geom_smooth(method = 'lm', se = FALSE, color = 'skyblue') +
  geom_point() +
  xlab('Age') +
  ylab('Simulator Sickness Score (Post - Pre)')
```

## Simulator sickness and age

<center>
<img src="images/simulator-sickness-jasp.png" />
</center>

- What did we actually do? What is our <span style="color: red">statistical model?</span>

## Simulator sickness and age
- Propose as statistical model a multivariate Gaussian distribution for the standardized variable $X = [x_1, x_2]$

$$
f(x|\Sigma) = \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1} \exp \big (\frac{1}{2}x^T\Sigma^{-1}x \big)
$$

```{r}
X <- ss_dat[, c('age', 'diff_ssq')]
Xs <- scale(X)
head(cbind(X, Xs))
```


## Bivariate Gaussian Distribution
```{r, echo = FALSE}
library('mvtnorm')

shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        tags$head(
          tags$script(src = mathjax_URL, type = 'text/javascript'),
          tags$script("MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});", type = 'text/x-mathjax-config')
    ),
        sliderInput("rho", label = HTML('$$\\rho$$'),
                    min = -1, max = 1, value = 0, step = 0.1)
        ),
      mainPanel(
        plotOutput("mvn_plot", height="500px", width="600px")
        )
      )
   )),
   server = function(input, output) {
      plot_dat <- function(rho) {
        mu <- c(0, 0)
        sigma <- matrix(c(1, rho, rho, 1), nrow = 2)
        X <- expand.grid(x1 = seq(-3, 3, length.out = 300), x2 = seq(-3, 3, length.out = 300))
        d <- cbind(X, prob = dmvnorm(X, mean = mu, sigma = sigma))

        ggplot(d, aes(x = x1, y = x2, z = prob)) +
          geom_contour() +
          coord_fixed(xlim = c(-3, 3), ylim = c(-3, 3), ratio = 1) +
          xlab(TeX('X_1')) +
          ylab(TeX('X_2')) +
          ggtitle(TeX(paste0('$\\rho = $', round(rho, 2)))) +
          theme(plot.title = element_text(hjust = .5))
      }
      
      output$mvn_plot <- renderPlot({
        plot_dat(input$rho)
      })
    }
)
```

## Simulator sickness and age
- The shape of the multivariate Gaussian is determined by the correlation matrix

$$
\Sigma = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}
$$
- We want to compare

$$
\begin{split}
H_0&: \rho = 0 \\
H_1&: \rho \sim \text{Beta}(1, 1)
\end{split}
$$


- For $M_0$, this is just the simple likelihood for $\rho = 0$

$$
\begin{split}
p(d|M_0) &= \frac{1}{\sqrt{2\pi}} \exp \big (-\frac{1}{2}x^T\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} ^{-1}x \big) \\[1ex]
         &\approx 8.414\mathrm{e}^{-35}
\end{split}
$$

```{r, echo = FALSE, eval = FALSE}
prod(dmvnorm(Xs, mean = c(0, 0), sigma = matrix(c(1, 0, 0, 1), nrow = 2)))
```

## Simulator sickness and age
- Again, for $M_1$ it's a bit more complicated as it involves integration over $\rho$

$$
\begin{split}
p(d|M_1) &= \int_{\rho} p(d, \rho|M_1)\mathrm{d}\rho \\[1ex]
         &= \int_{\rho} p(d|M_1, \rho)p(\rho|M_1)\mathrm{d}\rho \\[1ex]
         &= \int_{\rho} f(x;\Sigma)\text{Unif}(-1, 1)\mathrm{d}\rho \\[1ex]
         &= \int_{\rho} \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1} \exp \big (\frac{1}{2}x^T\Sigma^{-1}x \big) \frac{1}{2} \mathrm{d}\rho \\[1ex]
         &\approx \frac{1}{N} \sum_{\rho_i \sim \text{Unif}(-1, 1)} ^N \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1} \exp \big (\frac{1}{2}x^T\begin{pmatrix} 1 & \rho_i \\ \rho_i & 1 \end{pmatrix}^{-1}x \big) \\[1ex]
         &\approx 4.278\mathrm{e}^{-35}
\end{split}
$$

```{r, echo = FALSE, eval = FALSE}
N <- 100000
rho <- runif(N, -1, 1)
f <- function(x, rho) dmvnorm(x, mean = c(0, 0), sigma = matrix(c(1, rho, rho, 1), nrow = 2))

marginal_likelihood <- 0
for (i in seq(N)) {
  rho_i <- rho[i]
  marginal_likelihood <- marginal_likelihood + prod(f(Xs, rho_i))
}

marginal_likelihood / N
```

## Simulator sickness and age
$$
\text{BF}_{01} = \frac{8.414\mathrm{e}^{-35}}{4.278\mathrm{e}^{-35}} = 1.966
$$

<center>
<img src="images/simulator-sickness-jasp.png" />
</center>

## Harry Potter Sorting Hat Quiz
![](images/descriptives-big5.png)

## Harry Potter Sorting Hat Quiz
$$
\begin{split}
M_0 &: \mu_{\text{G}} = \mu_{\text{S}} = \mu_{\text{R}} = \mu_{\text{H}} \\
M_f &: \mu_{\text{G}}, \, \mu_{\text{S}} , \, \mu_{\text{R}} , \, \mu_{\text{H}} \\
M_r &: \mu_{\text{G}} > (\mu_{\text{S}} , \, \mu_{\text{R}} , \, \mu_{\text{H}})
\end{split}
$$

- Trick to compute the Bayes factor ...

## Markov chain Monte Carlo with People
![](images/MCMCP-model.png)


# Practicals with JASP

## JASP
- Undergoes rapid development in Eric-Jan Wagenmakers' lab in Amsterdam
- Has over 20 team members coming from psychology, statistics, and computer science
- Its aim is to supersede SPSS and provide Bayesian analysis to the world


## Current analyses
```{r, results = 'asis', echo = FALSE}
analyses <- rbind(
  c('Descriptives', '-', '-'),
  c('One Sample T-Test', 'Yes', 'Yes'),
  c('Paired Samples T-Test', 'Yes', 'Yes'),
  c('Independent Samples T-Test', 'Yes', 'Yes'),
  c('ANOVA', 'Yes', 'Yes'),
  c('ANCOVA', 'Yes', 'Yes'),
  c('Repeated Measures ANOVA', 'Yes', 'Yes'),
  c('Linear Regression', 'Yes', 'Yes'),
  c('Correlation', 'Yes', 'Yes')
)

papaja::apa_table(
  analyses,
  caption = 'Analyses available in JASP 0.8.1.',
  col.names = c('Analysis', 'Bayesian', 'Frequentist')
)
```

## Current analyses
```{r, results = 'asis', echo = FALSE}
analyses <- rbind(
  c('Binomial Test', 'Yes', 'Yes'),
  c('Contingency Tables', 'Yes', 'Yes'),
  c('Log-Linear Regression', 'Yes', 'Yes'),
  c('Summary Stats', 'Yes', '-'),
  c('Reliability Analysis', 'Not yet', 'Yes'),
  c('Principal Component Analysis', 'Not yet', 'Yes'),
  c('Exploratory Factor Analysis', 'Not yet', 'Yes'),
  c('Structural Equation Modeling', 'Not yet', 'Yes')
)

papaja::apa_table(
  analyses,
  caption = 'Analyses available in JASP 0.8.1.',
  col.names = c('Analysis', 'Bayesian', 'Frequentist')
)
```

## Correlation in JASP
<center>
  <img src="images/jasp-correlation.png" />
</center>

## Correlation in JASP
- **1** Load *Presidents.csv* into JASP. Run descriptive statistics and summarize what you find.

<span style="color: white"></span>

- **2** Conduct a classical correlation test.
    - **2a** Interpret the resulting *p*-value and confidence interval.
    - **2b** Are these concepts valid for the current data set? *Hint:* think about the data generating process.
    
<span style="color: white"></span>

- **3** Conduct a Bayesian correlation test.
    - **3a** Estimate the posterior distribution and interpret the Bayes factor.
    - **3b** Does the default prior in JASP make sense?
    - **3c** Run a robustness check. What do you conclude?
    - **3d** Run a sequential analysis. What do you conclude?

## t-test in JASP
<div style = "float:left; width:45%;">
    <img src="images/jasp-kitchen-rolls.png" />
</div>
<div style = "float:right; width:45%;">
    <img src="images/jasp-t-test.png" />
</div>


## t-test in JASP
- **1** Load *KitchenRolls.csv* into JASP. Run descriptive statistics and summarize the data graphically.

<span style="color: white"></span>

- **2** Conduct a classical t-test.
    - **2a** Interpret the resulting *p*-value and confidence interval.
    - **2b** Are these concepts valid for the current data set? *Hint:* think about the data generating process.
    
<span style="color: white"></span>
    
- **3** Conduct a Bayesian t-test.
    - **3a** Estimate the posterior distribution and interpret the Bayes factor.
    - **3b** Does the default prior in JASP make sense?
    - **3c** Run a robustness check. What do you conclude?
    - **3d** Run a sequential analysis. What do you conclude?
    
<span style="color: white"></span>

- **4** Conduct a Bayesian one-sided t-test. Is this approach justifiable?

## JASP wrap-up
<center>
  <img src="images/desiderata.png" />
</center>

# Final thoughts