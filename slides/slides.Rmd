---
title: "Notes on Statistics from a Bayesian Perspective"
subtitle: "A Conceptual and Practical Introduction"
author: "Fabian Dablander"
# runtime: shiny
bibliography: bibliography.bib
output:
  ioslides_presentation:
    # css: mistyle.css
    smaller: yes
    transition: faster
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, dev.args = list(bg = 'transparent'), fig.align = 'center')
```

\usepackages{csquotes}
\newcommand{\param}{\mathbf{\theta}}
\newcommand{\dat}{\textbf{y}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

## Motivation
- How old is statistics?
- Who invented the *p* value?
- What is Bayes' theorem?


## Outline
- **1a.** History of statistics
    - Relation to science
    - Controversies and personal hatred
    
<span style = "color:white"></span>

- **1b.** Introduction to probability
    - Frequentist versus Bayesian motivation

<span style = "color:white"></span>
    
- **2.** Statistical models
    - Example: Murphy's law
    - (Re)introduction *p*-values, confidence intervals, statistical power
    

## Outline
- **3.** Introduction to Bayesian statistics
    - Sum and product rule of probability
    - Parameter estimation, model comparison, model prediction
    
<span style = "color:white"></span>

- **4a.** Introducing JASP
    - Worked examples
    - Practice session
    
<span style = "color:white"></span>

- **4b.** Outlook
    - Where is Bayes going?
    - Where is JASP going?
    - Where are *you* going?
    
    

# Introduction to Probability
## Detective example
```{r, echo = FALSE, height = 3, width = 3, results = 'asis'}
library('papaja')

tab <- rbind(c(1/2, 1/4, 1/4), c(1/3, 1/3, 1/3))
apa_table(
  tab,
  caption = 'Probabilities',
  col.names = c('Self', 'Wife', 'Lover')
)
```


# Statistical models

## Some notation
- Let $X$ and $Y$ denote *random variables*
- greek letters like $\mu$ and $\sigma^2$ denote model parameters
- $X \sim \text{N}(\mu, \sigma^2)$ means $X$ is distributed normally with mean $\mu$ and variance $\sigma^2$
- $5 \approx 4$ means 5 is approximately 4
- $\sum$ and $\int$ denote summation and integration, respectively
- $\mathbb{E}[X]$ denotes the expectation or mean of the random variable $X$; $\mathbb{Var}[X]$ its variance
- $f(X = x; \mu)$ will denote the likelihood of a value given the parameter $\mu$
- $P(X = x|\theta)$ denotes the (discrete) conditional probability of $X = x$ given $\theta$
- $p(X = x|\theta)$ denotes the (continuous) conditional probability of $X = x$ given $\theta$
- $p(X = x)$ denotes the marginal probability of $X = x$


## Statistical models
- say I have $n = 100$ observations $x = \{x_1, x_2, x_3, \ldots, x_n\}$
- you ask me about the data; what can I tell you?
```{r, echo = FALSE}
library('ggplot2')
theme_set(papaja::theme_apa())

set.seed(1774)
x <- rnorm(100, 100, 10)
```

```{r}
x
```

## Statistical models
- I introduce a statistical model to reduce complexity
- A statistical model describes the relationship of one or more **latent parameters** to the observations

```{r, echo = FALSE}
mu <- mean(x)
sigma <- sd(x)

ggplot(data.frame(x = x), aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 20, fill = 'grey46', col = 'black') +
  stat_function(fun = function(y) dnorm(y, mean = mu, sd = sigma), col = 'firebrick', size = 1.5)
```

## Statistical models
- I used a normal distribution, assuming that
$$
f(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \big (-\frac{1}{2\sigma} (x - \mu)^2 \big)
$$
- now I can describe the data with just two numbers
```{r}
c(mean(x), sd(x))
```

## Statistical models
- Also called **likelihood functions**
- Note that our statistical model need be good; it can be horribly misspecified!

$$
f(x; a, b) = \frac{1}{b - a}
$$
```{r, echo = FALSE}
ggplot(data.frame(x = x), aes(x = x)) +
  geom_histogram(aes(y = ..density..), bins = 20, fill = 'grey46', col = 'black') +
  stat_function(fun = function(y) dunif(y, min = 50, max = 150), col = 'firebrick', size = 1.5)
```


## Statistical models: Practical example
- Over the last few weeks, bread fell sometimes with the buttery side on the floor
- on other occasions, it fell on the floor with the dry side
- Questions
    - <span style = "color:red">What is the propensity of the bread to fall on the floor with the butter side down?</span>
    - <span style = "color:red">How do I describe this process? Any suggestions?</span>


## Statistical models: Bread and butter
- First, note that the outcome is binary, $x = \{0, 1\}$
- Let's introduce a latent parameter, $\theta$
- A good, tried suggestion would be the following statistical model

$$
\begin{align*}
f(x; \theta) &= \theta^x (1 - \theta)^{1 - x} \\[2ex]
f(x; \theta) &= \begin{cases} \theta & \text{if } x = 1 \\ 1 - \theta & \text{otherwise} \end{cases}
\end{align*}
$$

## Statistical models: Bread and butter
- with this function I can describe one event
- so how do I describe $n$ events?
- I make a <span style = "color:red">very contentious assumption</span>: **independent and identically distributed samples**
- this allows me to just multiply the individual samples

## Statistical models: Bread and butter
- assume I have $n = 20$ data points $x = \{x_1, x_2, x_3, \ldots, x_n\}$

$$
\begin{align*}
f(x; \theta) &\stackrel{\text{i.i.d.}}{=} f(x_1; \theta) \cdot f(x_2; \theta) \cdot \ldots \cdot f(x_n; \theta) \\
             &= \prod^n_{i=1} f(x_i; \theta) \\
             &= \prod^n_{i=1} \theta^{x_i} (1 - \theta)^{1 - x_i} \\
             &= \theta^{\sum^n_{i = 1} x_i} (1 - \theta)^{\sum^n_{i = 1} 1 - x_i}
\end{align*}
$$

## Side note: Binomial model
- I don't care about the order of the bread falls
- i.e., I don't care whether $x = \{1, 0 \}$ or $x = \{0, 1\}$
- Neglecting the order, we only model the number of falls with butter on the floor

$$
Y = \sum_{i=1}^n x_i
$$

- noting that there are:

$$
{n \choose y} = \frac{n!}{y!(n - y)!}
$$

- possible ways of obtaining $Y = y$ successes yields the **Binomial** model


## Side note: Binomial model
- We don't need the exact information of $x = \{x_1, x_2, x_3, \ldots, x_n\}$
- $y$ and $n$ are *sufficient* to describe the data
- The likelihood function changes to be

$$
\begin{align*}
f(y; \theta, n) &= {n \choose y} \theta^{\sum^n_{i = 1} x_i} (1 - \theta)^{\sum^n_{i = 1} 1 - x_i} \\[1ex]
                &= {n \choose y} \theta^{y} (1 - \theta)^{n - y} \\
\end{align*}
$$

    
## Statistical models: Bread and butter
- I observe the following data

```{r, echo = FALSE}
set.seed(1774)

x <- rbinom(20, 1, prob = .7)
y <- sum(x)
n <- length(x)

ggplot(data.frame(x = x), aes(x = x)) +
  geom_bar()
```

- I want to learn $\theta$ --- what are good values for it, given the data?


## Statistical models: Bread and butter
- Let's plot the data as a function of $\theta$

```{r, echo = FALSE}
ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(fun = function(theta) dbinom(sum(x), length(x), prob = theta)) +
  ylab('Likelihood') +
  xlab(expression(theta))
```

    
## Likelihoods
- Answer to: **How likely is the data given certain parameter values?**
- Let's compare $\theta = .5$ and $\theta = .75$ on our flipping data ($y = 15, n = 20$)

$$
\begin{align*}
f(Y = 15|n = 20, \theta = .5)  &= {20 \choose 15} .50^{15} (1 - .50)^{20 - 15} \approx 0.0147 \\[1ex]
f(Y = 15|n = 20, \theta = .75) &= {20 \choose 15} .75^{15} (1 - .75)^{20 - 15} \approx 0.2033
\end{align*}
$$

## Likelihoods
```{r, echo = FALSE}
p1 <- .5
p2 <- .75
llh.p1 <- dbinom(y, n, p1)
llh.p2 <- dbinom(y, n, p2)

ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(fun = function(theta) dbinom(sum(x), length(x), prob = theta)) +
  geom_segment(x = p1, y = -1, xend = p1, yend = llh.p1, col = 'red', linetype = 'dashed') +
  geom_segment(x = p2, y = -1, xend = p2, yend = llh.p2, col = 'blue', linetype = 'dashed') +
  geom_point(x = p1, y = llh.p1, size = 2) +
  geom_point(x = p2, y = llh.p2, size = 2) +
  ylab('Likelihood') +
  xlab(expression(theta))
```

## Likelihood ratios
```{r, echo = FALSE, height = 3, width = 3}
library('shiny')

shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        sliderInput("p1", label = "p1",
                    min = 0, max = 1, value = 0.7, step = 0.01),
        sliderInput("p2", label = "p2",
                    min = 0, max = 1, value = 0.5, step = 0.01),
        sliderInput("k", label = "k",
                    min = 1, max = 50, value = 7, step = 1),
        sliderInput("N", label = "N",
                    min = 1, max = 50, value = 10, step = 1),
        br(),
        textOutput("LRatio1")
        ),
      mainPanel(
        plotOutput("LRplot", height="400px")
        )
      )
   )),
   server = function(input, output) {
      plot.LR <- function(k, N, p1, p2) {
        # # adapted from blog mentioned above
        # MLE <- dbinom(k, N, k/N)
        # L1 <- dbinom(k, N, prob = p1) / MLE
        # L2 <- dbinom(k, N, prob = p2) / MLE
        # 
        # curve((dbinom(k, N, x) / max(dbinom(k, N, x))), xlim = c(0, 1),
        #       ylab = "Likelihood", xlab = "Probability of correct answer",
        #       las = 1, main = "Likelihood function for binomials", lwd = 3,
        #       cex.axis = 1.5, cex.lab = 1.5, cex.main = 1.5)
        # 
        # points(p1, L1, cex = 2, pch = 21, bg = "cyan")
        # points(p2, L2, cex = 2, pch = 21, bg = "cyan")
        # lines(c(p1, p2), c(L1, L1), lwd = 3, lty = 2, col = "cyan")
        # lines(c(p2, p2), c(L1, L2), lwd = 3, lty = 2, col = "cyan")
        # abline(v = k/N, lty = 5, lwd = 1, col = "grey73")
        llh.p1 <- dbinom(k, N, p1)
        llh.p2 <- dbinom(k, N, p2)
        
        ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
          stat_function(fun = function(theta) dbinom(sum(x), length(x), prob = theta)) +
          geom_segment(x = p1, y = -1, xend = p1, yend = llh.p1, col = 'red', linetype = 'dashed') +
          geom_segment(x = p2, y = -1, xend = p2, yend = llh.p2, col = 'blue', linetype = 'dashed') +
          geom_point(x = p1, y = llh.p1, size = 2) +
          geom_point(x = p2, y = llh.p2, size = 2) +
          ylab('Likelihood') +
          xlab(expression(theta))
      }
      
      get.LR <- function(k, N, p1, p2) {
        MLE <- dbinom(k, N, k/N)
        L1 <- dbinom(k, N, prob = p1) / MLE
        L2 <- dbinom(k, N, prob = p2) / MLE
        L1 / L2
      }
      
      output$LRplot <- renderPlot({
        k <- input$k
        N <- input$N
        if (k <= N) plot.LR(k, N, input$p1, input$p2)
      })
      
      output$LRatio1 <- renderText({
        LR_12 <- get.LR(input$k, input$N, input$p1, input$p2)
        paste("L1 / L2: ", round(LR_12, 3))
      })
      
      output$LRatio2 <- renderText({
        LR_21 <- 1 / get.LR(input$k, input$N, input$p1, input$p2)
        paste("L2 / L1: ", round(LR_21, 3))
      })
    }
)
```



## Classical statistics
- is what you learned in class
- parameters are unknown, fixed quantities
- uses the notions of
    - repeated sampling
    - Type I and Type II errors
    - confidence intervals
    - *p* values
    - statistical power


## Confidence intervals
```{r, echo = FALSE}
simulate.cis <- function(y, n, times = 100) {
    p <- y / n
    cis <- matrix(NA, nrow = times, ncol = 2)
    
    for (i in 1:times) {
        y <- sample(c(1, 0), n, replace = TRUE, prob = c(p, 1-p))
        
        p.hat <- mean(y)
        se <- sd(y) / sqrt(n)
        
        cis[i, 1] <- p.hat - 1.96 * se
        cis[i, 2] <- p.hat + 1.96 * se
    }
    
    within <- apply(cis, 1, function(row) row[1] < p && row[2] > p)
    cbind(cis, within)
}

plot.cis <- function(cis, times) {
    x <- 0:times
    y <- seq(0, 1, 1/times)
    
    plot(x, y, type = 'n', xlab = 'i-th repeated sample',
         ylab = 'bias', main = '95% CIs for bias estimate')
    
    abline(18/24, 0, lwd = 3)
    
    arrows(x0 = x, y0 = cis[, 1], x1 = x, length = 0, lwd = 1.2,
           y1 = cis[, 2], col = ifelse(cis[, 3], 'black', 'red'))
}

times <- 100
cis <- simulate.cis(18, 24, times = times)
```

```{r, echo = TRUE}
plot.cis(cis, times)
```


# Bayesian probability

## Bayesian probability
- Detective example!
- probability as a (normative) measure of degree of belief
- probability as an extension of logic to include inductive inferences
- concretely, what does this mean for the coin flip example?
    - is it *not* the proportion of heads in an infinite series?
\
\
- **intuition**: what if the coin has two heads or two tails?
    - you would still predict $P(\text{heads}) = .5$ for the first flip

## Rules of probability

$$
\begin{align}
P(H, D) &= P(H|D)P(D) = P(D|H)P(H) \\[2ex]
P(D) &= \sum_{H} P(H, D) = \sum_{H} P(D|H)P(H)
\end{align}
$$

- arrive at Bayes' rule by simple rearrangement

$$
\begin{align*}
P(H, D) &= P(H, D) \\
P(H|D)P(D) &= P(D|H)P(H) \\
P(H|D) &= \frac{P(D|H)P(H)}{P(D)} = \frac{P(D|H)P(H)}{\sum_{H} P(D|H)P(H)}
\end{align*}
$$


## Bayesian updating
- add a graphical model representation

## Bayesian updating
```{r, echo = FALSE, height = 3, width = 3}
library('shiny')
# the code (with a little cleaning up) for the visualisations is from
# http://alexanderetz.com/2015/07/25/understanding-bayes-updating-priors-via-the-likelihood/
# Alex Etz runs a really nice blog -- go check it out!

shinyApp(
  options = list(width = "25%", height = "25%"),
  ui = shinyUI(fluidPage(
    sidebarLayout(
      sidebarPanel(
        sliderInput("a", label = "a", min = 1, max = 50, value = 1, step = 1),
        sliderInput("b", label = "b", min = 1, max = 50, value = 1, step = 1),
        sliderInput("k", label = "k", min = 1, max = 50, value = 7, step = 1),
        sliderInput("N", label = "N", min = 1, max = 50, value = 10, step = 1),
        htmlOutput("BF_10")
        ),
      mainPanel(
        plotOutput('update_plot', height='400px')
        )
      )
    )
  ),
  server = function(input, output) {
        update_plot <- function(a = 1, b = 1, k = 0, N = 0, null = NULL, CI = NULL, ymax = 'auto') {
          x <- seq(.001, .999, .001) ## set up for creating the distributions
          y1 <- dbeta(x, a, b) # data for prior curve
          y3 <- dbeta(x, a + k, b + N - k) # data for posterior curve
          y2 <- dbeta(x, 1 + k, 1 + N - k) # data for likelihood curve, plotted as the posterior from a beta(1,1)
          y.max <- ifelse(is.numeric(ymax), ymax, 1.25 * max(y1, y2, y3, 1.6))
          title <- paste0('Beta(', a, ', ', b, ')', ' to Beta(', a + k, ', ', b + N - k, ')')
          
          plot(x, y1, xlim = c(0, 1), ylim = c(0, y.max), type = 'l', ylab = 'Density', lty = 2,
               xlab = 'Probability of success', las = 1, main = title, lwd=3,
               cex.lab = 1.5, cex.main = 1.5, col = 'skyblue', axes = FALSE)
          
          axis(1, at = seq(0, 1, .2)) #adds custom x axis
          axis(2, las = 1) # custom y axis
          
          if (N != 0) {
              # if there is new data, plot likelihood and posterior
              lines(x, y2, type = 'l', col = 'darkorange', lwd = 2, lty = 3)
              lines(x, y3, type = 'l', col = 'darkorchid1', lwd = 5)
              legend('topleft', c('Prior', 'Posterior', 'Likelihood'),
                     col = c('skyblue', 'darkorchid1', 'darkorange'), 
                     lty = c(2, 1, 3), lwd = c(3, 5, 2), bty = 'n',
                     y.intersp = 1, x.intersp = .4, seg.len =.7)
                  
              ## adds null points on prior and posterior curve if null is specified and there is new data
              if (is.numeric(null)) {
                      ## Adds points on the distributions at the null value if there is one and if there is new data
                      points(null, dbeta(null, a, b), pch = 21, bg = 'blue', cex = 1.5)
                      points(null, dbeta(null, a + k, b + N - k), pch = 21, bg = 'darkorchid', cex = 1.5)
                      abline(v=null, lty = 5, lwd = 1, col = 'grey73')
                      ##lines(c(null,null),c(0,1.11*max(y1,y3,1.6))) other option for null line
                }
          }
          
          ## Specified CI% but no null? Calc and report only CI
          if (is.numeric(CI) && !is.numeric(null)) {
                CI.low <- qbeta((1 - CI)/2, a + k, b + N - k)
                CI.high <- qbeta(1 - (1 - CI)/2, a + k, b + N - k)
                
                SEQlow <- seq(0, CI.low, .001)
                SEQhigh <- seq(CI.high, 1, .001)
                
                ## Adds shaded area for x% Posterior CIs
                cord.x <- c(0, SEQlow, CI.low) ## set up for shading
                cord.y <- c(0, dbeta(SEQlow, a + k, b + N - k), 0) ## set up for shading
                polygon(cord.x, cord.y, col='orchid', lty= 3) ## shade left tail
                cord.xx <- c(CI.high, SEQhigh, 1) 
                cord.yy <- c(0, dbeta(SEQhigh, a + k, b + N - k), 0)
                polygon(cord.xx, cord.yy, col='orchid', lty=3) ## shade right tail
                return(list('Posterior CI lower' = round(CI.low, 3),
                            'Posterior CI upper' = round(CI.high, 3)))
          }
          
          ## Specified null but not CI%? Calculate and report BF only 
          if (is.numeric(null) && !is.numeric(CI)){
              null.H0 <- dbeta(null, a, b)
              null.H1 <- dbeta(null, a + k, b + N - k)
              CI.low <- qbeta((1 - CI)/2, a + k, b + N - k)
              CI.high <- qbeta(1 - (1 - CI)/2, a + k, b + N - k)
              return(list('BF01 (in favor of H0)' = round(null.H1/null.H0, 3),
                          'BF10 (in favor of H1)' = round(null.H0/null.H1, 3)))
          }
          
          ## Specified both null and CI%? Calculate and report both
          if (is.numeric(null) && is.numeric(CI)){
                  null.H0 <- dbeta(null, a, b)
                  null.H1 <- dbeta(null, a + k, b + N - k)
                  CI.low <- qbeta((1 - CI)/2, a + k, b + N - k)
                  CI.high <- qbeta(1 - (1 - CI)/2, a + k, b + N - k)
                  
                  SEQlow <- seq(0, CI.low, .001)
                  SEQhigh <- seq(CI.high, 1, .001)
                  
                  ## Adds shaded area for x% Posterior CIs
                  cord.x <- c(0, SEQlow, CI.low) ## set up for shading
                  cord.y <- c(0, dbeta(SEQlow, a + k, b + N - k), 0) ## set up for shading
                  polygon(cord.x, cord.y, col = 'orchid', lty = 3) ## shade left tail
                  cord.xx <- c(CI.high, SEQhigh, 1) 
                  cord.yy <- c(0, dbeta(SEQhigh, a + k, b + N - k), 0)
                  polygon(cord.xx, cord.yy, col = 'orchid', lty = 3) ## shade right tail
                  return(list('BF01 (in favor of H0)' = round(null.H1/null.H0, 3),
                              'BF10 (in favor of H1)' = round(null.H0/null.H1, 3),
                              'Posterior CI lower' = round(CI.low, 4),
                              'Posterior CI upper' = round(CI.high, 3)))
          }
        }
        output$update_plot <- renderPlot({
          null <- .5
          update_plot(input$a, input$b, input$k, input$N, null = null)
        })
        
        output$BF_10 <- renderText({
          a <- input$a
          b <- input$b
          k <- input$k
          N <- input$N
          label <- 'BF<span style="font-size: .6em;">10</span>: '
          BF10 <- 1 / (dbeta(.5, a + k, b + N - k) / dbeta(.5, a, b))
          paste(label, round(BF10, 3))
        })
  }
)
```


## Bayesian testing
- **estimation pre-supposes existence**
- it is a conceptual error to use estimation for testing (e.g., psi)
    - have to specify a 'lump of probability' for the general law
    - and then test against this point value, or null model

## Bayesian testing
- same coin flip data: $\{1, 1, 1\}$

$$
f(Y = 3|n = 3, \theta = .5) = {3 \choose 3} .5^3 (1 - .5)^{3 - 3} = 0.125 \\
f(Y = 3|n = 3, \theta = .8) = {3 \choose 3} .8^3 (1 - .8)^{3 - 3} = 0.512
$$

- evidence of $\theta = .8$ **versus** $\theta = .5$ is $.512 / .125 = 4.096$

## Bayesian testing
- comparing point likelihoods is too demanding
    - requires alternative specified as a point
    - we aren't really that certain of our prediction!
\
\
- Bayes solution
    - specify a distribution to quantify this uncertainty
    - gives the **Bayes factor**


## Bayes factor
- $M_0: \theta = .5$ versus $M_1: \theta \sim \mathcal{Beta(1, 1)}$
- pit the predictive power of the two models against each other
    - $p(D|M_0)$ versus $p(D|M_1)$
    
\
\
- recall Bayes' rule, but conditioning on the model
$$
p(\theta|D, M_x) = \frac{p(D|\theta, M_x)p(\theta|M_x)}{p(D|M_x)}
$$

- where

$$
p(D|M_x) = \int p(D, \theta|M_x) \mathrm{d}\theta = \int p(D|\theta, M_x)p(\theta|M_x) \mathrm{d}\theta
$$

## Bayes factor
- update our beliefs about the models using ... Bayes' rule
\
\
$$
\underbrace{\frac{P(M_1 \mid D)}{P(M_2 \mid D)}}_{\text{posterior odds}} =
\underbrace{\frac{P(D \mid M_1)}{P(D \mid M_2)}}_{\text{Bayes factor}} \, \cdot \underbrace{\frac{P(M_1)}{P(M_2)}}_{\text{prior odds}}
$$


## Bayes factor
- is the coin fair?
    - $M_0: \theta = .5$ versus $M_1: \theta \sim \mathcal{Beta(1, 1)}$
\
\
- the marginal likelihoods become

$$
\begin{align}
p(D|M_0) &= {3 \choose 3} .5^3 (1 - .5)^{3 - 3} = .125 \\
p(D|M_1) &= \int p(D, \theta|M_1)\mathrm{d}\theta = \int p(D|\theta, M_1)p(\theta|M_1) \mathrm{d}\theta
\end{align}
$$

## Bayes factor
- is the non-plus-ultra of model comparison
    - complex models can predict many data patterns
    - thus spread out their prior probability
    - this gets factored into the marginal likelihood and decreases it
    - instantiates an **automatic Ockham's razor**
\
\
- looks at the functional form of the model, compared to
    - $AIC = -2 \log p(\textbf{y}|\hat \theta) + 2\cdot k$
    - $BIC = -2 \log p(\textbf{y}|\hat \theta) + \log n \cdot k$


## Bayes factor
- can compare non-nested models
- can support the null hypothesis
- its interpretation does not depend on the sample size
- can sequentially test participants, without $p$-hacking
\
\
- use software like JASP or BayesFactor
    - use a **default prior** approach pioneered by Harold Jeffreys
    
## Lindley's Paradox
```{r}
k <- 49581
N <- 98451
```

<div style = "float:left; width:45%;">
**$p$-value**

```{r}
binom.test(k, N)$p.value
```
</div>
<div style = "float:right; width:45%;">

**Bayes factor (Savage-Dickey)**

```{r}
dbeta(0.5, k + 1, N - k + 1)
```  
</div>

# JASP

## Current analyses
```{r, results = 'asis', echo = FALSE}
analyses <- rbind(
  c('Descriptives', '-', '-'),
  c('One Sample T-Test', 'Yes', 'Yes'),
  c('Paired Samples T-Test', 'Yes', 'Yes'),
  c('Independent Samples T-Test', 'Yes', 'Yes'),
  c('ANOVA', 'Yes', 'Yes'),
  c('ANCOVA', 'Yes', 'Yes'),
  c('Repeated Measures ANOVA', 'Yes', 'Yes'),
  c('Linear Regression', 'Yes', 'Yes'),
  c('Correlation', 'Yes', 'Yes')
)

papaja::apa_table(
  analyses,
  caption = 'Analyses available in JASP 0.8.1.',
  col.names = c('Analysis', 'Bayesian', 'Frequentist')
)
```

## Current analyses
```{r, results = 'asis', echo = FALSE}
analyses <- rbind(
  c('Binomial Test', 'Yes', 'Yes'),
  c('Contingeny Tables', 'Yes', 'Yes'),
  c('Log-Linear Regression', 'Yes', 'Yes'),
  c('Summary Stats', 'Yes', 'No'),
  c('Reliability Analysis', 'No', 'Yes'),
  c('Principal Component Analysis', 'No', 'Yes'),
  c('Exploratory Factor Analysis', 'No', 'Yes'),
  c('Structural Equation Modeling', 'No', 'Yes')
)

papaja::apa_table(
  analyses,
  caption = 'Analyses available in JASP 0.8.1.',
  col.names = c('Analysis', 'Bayesian', 'Frequentist')
)
```

## Under Development
- Machine learning module
- Informative hypothesis testing module

## Future Additions
- Multilevel models